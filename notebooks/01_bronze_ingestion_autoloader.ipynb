{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48025a68-3f1f-44ee-b2f8-48d4cdffcef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ------------------------------------------------------------------------------\n",
    "# Notebook: 01_bronze_ingestion_autoloader\n",
    "# Purpose : Ingest Chicago Taxi raw data into the bronze layer as Delta by path,\n",
    "#           adding ingestion metadata and simulating Auto Loader / COPY INTO patterns.\n",
    "#\n",
    "# Exam Coverage (Databricks Certified Data Engineer Associate – Exam Guide 2025-07-30)\n",
    "# - Section 2: Development and Ingestion\n",
    "#   - Capabilities of notebooks for data engineering workflows.\n",
    "#   - Auto Loader sources, use cases and syntax (conceptual in CE).\n",
    "#   - COPY INTO as an ingestion pattern from cloud storage into Delta tables.\n",
    "# - Section 3: Data Processing & Transformations\n",
    "#   - Role of the bronze layer in a medallion pipeline.\n",
    "# - Section 5: Data Governance & Quality\n",
    "#   - Ingestion-time metadata columns and basic sanity checks.\n",
    "#\n",
    "# Key Practices\n",
    "# - Read CSV with an explicit schema.\n",
    "# - Write Delta by path under /Volumes/{catalog}/{schema}/{volume}/bronze/.\n",
    "# - Add ingestion_ts and capture _metadata.file_path when available.\n",
    "# - Keep ingestion idempotent for demos (overwrite) and document incremental options.\n",
    "# ------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e325577-0138-4898-91d1-b8cb12320c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exam Focus – Ingestion Patterns (Section 2)\n",
    "\n",
    "This notebook reinforces key ingestion patterns for the exam:\n",
    "\n",
    "- Manual batch ingestion using `spark.read` + `DataFrame.write.format(\"delta\")`.\n",
    "- Conceptual understanding of **COPY INTO** to load files into Delta tables.\n",
    "- Conceptual understanding of **Auto Loader** for incremental file ingestion.\n",
    "\n",
    "Key exam ideas:\n",
    "- **COPY INTO**:\n",
    "  - SQL command to load data from cloud storage locations into a Delta table.\n",
    "  - Good for repeatable, idempotent batch loads.\n",
    "- **Auto Loader**:\n",
    "  - Uses `cloudFiles` to incrementally track new files from cloud storage.\n",
    "  - Handles schema inference and schema evolution.\n",
    "- In Community Edition, these features might not be available, but knowing their\n",
    "  syntax and use cases is important for the exam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b716b5c-37f6-4b7f-aeae-1d1aa814e452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "816c9e79-4a9c-455f-b8c9-0929a68d02e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widgets\n",
    "dbutils.widgets.text(\"catalog\", \"taxi_catalog\")\n",
    "dbutils.widgets.text(\"schema\",  \"taxi_schema\")\n",
    "dbutils.widgets.text(\"volume\",  \"taxi_volume\")\n",
    "\n",
    "# Read widget values\n",
    "catalog_name = dbutils.widgets.get(\"catalog\")\n",
    "schema_name  = dbutils.widgets.get(\"schema\")\n",
    "volume_name  = dbutils.widgets.get(\"volume\")\n",
    "\n",
    "# Base & medallion paths\n",
    "base_path   = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}\"\n",
    "bronze_path = f\"{base_path}/bronze\"\n",
    "silver_path = f\"{base_path}/silver\"\n",
    "gold_path   = f\"{base_path}/gold\"\n",
    "\n",
    "# Entities for bronze\n",
    "bronze_table_path = f\"{bronze_path}/chicago_taxi_trips\"\n",
    "bronze_view_name  = f\"{catalog_name}.{schema_name}.chicago_taxi_bronze_v\"\n",
    "\n",
    "print(\"Base:\", base_path)\n",
    "print(\"Bronze table path:\", bronze_table_path)\n",
    "print(\"Bronze view:\", bronze_view_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47e22261-8c1e-46ce-b090-d4f4a9a460ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure medallion directories exist in the volume\n",
    "dbutils.fs.mkdirs(bronze_path)\n",
    "dbutils.fs.mkdirs(silver_path)\n",
    "dbutils.fs.mkdirs(gold_path)\n",
    "\n",
    "print(\"Medallion directories ensured under base path\")\n",
    "display(dbutils.fs.ls(base_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b64eab00-ceb8-40d9-9660-0d8e2cc7d416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ensure medallion directories exist\n",
    "dbutils.fs.mkdirs(bronze_path); dbutils.fs.mkdirs(silver_path); dbutils.fs.mkdirs(gold_path)\n",
    "\n",
    "# Define and check source file in bronze\n",
    "raw_source_path = f\"{bronze_path}/m6dm-c72p.csv\"\n",
    "files = [f.path.replace(\"dbfs:\", \"\") for f in dbutils.fs.ls(bronze_path)]\n",
    "assert raw_source_path.replace(\"dbfs:\", \"\") in files, f\"Source file not found: {raw_source_path}\"\n",
    "print(\"OK - Source file:\", raw_source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5823b5-dc8a-4172-b903-916a2b34cbeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# 1) Explicit schema (avoid inference)\n",
    "taxi_schema = StructType([\n",
    "    StructField(\"taxi_id\", StringType(), True),\n",
    "    StructField(\"trip_start_timestamp\", TimestampType(), True),\n",
    "    StructField(\"trip_end_timestamp\", TimestampType(), True),\n",
    "    StructField(\"trip_seconds\", IntegerType(), True),\n",
    "    StructField(\"trip_miles\", DoubleType(), True),\n",
    "    StructField(\"pickup_census_tract\", StringType(), True),\n",
    "    StructField(\"dropoff_census_tract\", StringType(), True),\n",
    "    StructField(\"pickup_community_area\", IntegerType(), True),\n",
    "    StructField(\"dropoff_community_area\", IntegerType(), True),\n",
    "    StructField(\"fare\", DoubleType(), True),\n",
    "    StructField(\"tips\", DoubleType(), True),\n",
    "    StructField(\"tolls\", DoubleType(), True),\n",
    "    StructField(\"extras\", DoubleType(), True),\n",
    "    StructField(\"trip_total\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"company\", StringType(), True),\n",
    "    StructField(\"pickup_centroid_latitude\", DoubleType(), True),\n",
    "    StructField(\"pickup_centroid_longitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_centroid_latitude\", DoubleType(), True),\n",
    "    StructField(\"dropoff_centroid_longitude\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "# 2) Read CSV and add governance metadata (UC-safe: _metadata.file_path)\n",
    "df_raw = (spark.read.option(\"header\", True).schema(taxi_schema).csv(raw_source_path))\n",
    "df_bronze = (df_raw.withColumn(\"ingestion_ts\", current_timestamp())\n",
    "                  .withColumn(\"source_file\", col(\"_metadata.file_path\")))\n",
    "\n",
    "# 3) Write Delta by path (idempotent overwrite for demo)\n",
    "df_bronze.write.format(\"delta\").mode(\"overwrite\").save(bronze_table_path)\n",
    "\n",
    "# 4) Expose UC VIEW over Delta path (tables with LOCATION are blocked in CE+UC)\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE OR REPLACE VIEW {bronze_view_name} AS\n",
    "  SELECT * FROM delta.`{bronze_table_path}`\n",
    "\"\"\")\n",
    "\n",
    "# 5) Basic consistency check: path vs VIEW\n",
    "cnt_path = spark.sql(f\"SELECT COUNT(*) c FROM delta.`{bronze_table_path}`\").collect()[0][\"c\"]\n",
    "cnt_view = spark.sql(f\"SELECT COUNT(*) c FROM {bronze_view_name}\").collect()[0][\"c\"]\n",
    "print(\"Counts -> path:\", cnt_path, \"| view:\", cnt_view)\n",
    "assert cnt_path == cnt_view and cnt_path > 0, \"Bronze ingestion failed or empty.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d5a418-e449-4b31-8dd6-6556e6857c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "dt = DeltaTable.forPath(spark, bronze_table_path)\n",
    "display(dt.history(5))  # recent commits\n",
    "\n",
    "# Optional: try version 0\n",
    "try:\n",
    "    _ = (spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(bronze_table_path).limit(1).count())\n",
    "    print(\"Time travel OK (version 0)\")\n",
    "except Exception:\n",
    "    print(\"No earlier version available yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35f95b7-d00f-4a0d-8782-0c3713f717bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "bronze_df = spark.read.format(\"delta\").load(bronze_table_path)\n",
    "assert bronze_df.count() > 0, \"Empty bronze.\"\n",
    "\n",
    "# Expected columns present\n",
    "for c in [\"trip_total\",\"fare\",\"tips\",\"tolls\",\"extras\",\"trip_seconds\",\"trip_miles\"]:\n",
    "    assert c in bronze_df.columns, f\"Missing expected column: {c}\"\n",
    "\n",
    "# Non-negative monetary/time/distance\n",
    "viol = []\n",
    "for c in [\"trip_total\",\"fare\",\"tips\",\"tolls\",\"extras\",\"trip_seconds\",\"trip_miles\"]:\n",
    "    if bronze_df.filter(col(c) < 0).limit(1).count() > 0:\n",
    "        viol.append(c)\n",
    "assert not viol, f\"Negative values found in: {viol}\"\n",
    "print(\"Bronze DQ checks passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed97a2ca-8445-43a1-8ca3-829e96b8b4e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- COPY INTO pattern (exam concept, may not run on CE)\n",
    "COPY INTO taxi_catalog.taxi_schema.chicago_taxi_bronze\n",
    "FROM 's3://bucket/path/to/chicago_taxi/'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'false')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "-- Auto Loader pattern (exam concept, may not run on CE)\n",
    "-- STREAMING EXAMPLE (PySpark pseudo-code, exam-style)\n",
    "-- spark.readStream.format(\"cloudFiles\") \\\n",
    "--   .option(\"cloudFiles.format\", \"csv\") \\\n",
    "--   .option(\"cloudFiles.inferColumnTypes\", \"true\") \\\n",
    "--   .load(\"s3://bucket/path/to/chicago_taxi/\") \\\n",
    "--   .writeStream \\\n",
    "--   .option(\"checkpointLocation\", \"s3://bucket/checkpoints/chicago_taxi/\") \\\n",
    "--   .table(\"taxi_catalog.taxi_schema.chicago_taxi_bronze\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b850645-35e2-4a22-89a6-714e9474a191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bronze preflight: verify the Delta path exists and is readable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "try:\n",
    "    _ = dbutils.fs.ls(bronze_table_path)  # folder must exist\n",
    "    _ = spark.read.format(\"delta\").load(bronze_table_path).limit(1).count()  # table must be readable\n",
    "    print(\"Bronze preflight: OK ->\", bronze_table_path)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Bronze not ready at {bronze_table_path}. Run the bronze ingestion cells first. Original: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7b31d6-f9c5-4ee9-86c1-de9a2763e528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Signal success to orchestrator\n",
    "dbutils.notebook.exit(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d78e56b8-1bb8-47aa-bec0-67ffa4e167ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_ingestion_autoloader",
   "widgets": {
    "catalog": {
     "currentValue": "taxi_catalog",
     "nuid": "b547edd3-14df-42ea-957c-6151df60dec4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "taxi_catalog",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "taxi_catalog",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "taxi_schema",
     "nuid": "d7716ca5-2bd3-4f46-9a32-13ee6e13dec5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "taxi_schema",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "taxi_schema",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volume": {
     "currentValue": "taxi_volume",
     "nuid": "b206f009-275e-4fcc-b095-bed015666ea2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "taxi_volume",
      "label": null,
      "name": "volume",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "taxi_volume",
      "label": null,
      "name": "volume",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
